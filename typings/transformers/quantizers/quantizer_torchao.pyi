"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Dict, List, TYPE_CHECKING, Union
from .base import HfQuantizer
from ..modeling_utils import PreTrainedModel
from ..utils import is_torch_available, is_torchao_available

if TYPE_CHECKING:
    ...
if is_torch_available():
    ...
if is_torchao_available():
    ...
logger = ...
def find_parent(model, name):
    ...

class TorchAoHfQuantizer(HfQuantizer):
    """
    Quantizer for torchao: https://github.com/pytorch/ao/
    """
    requires_parameters_quantization = ...
    requires_calibration = ...
    required_packages = ...
    def __init__(self, quantization_config, **kwargs) -> None:
        ...
    
    def validate_environment(self, *args, **kwargs): # -> None:
        ...
    
    def update_torch_dtype(self, torch_dtype): # -> dtype:
        ...
    
    def adjust_target_dtype(self, target_dtype: torch.dtype) -> torch.dtype:
        ...
    
    def adjust_max_memory(self, max_memory: Dict[str, Union[int, str]]) -> Dict[str, Union[int, str]]:
        ...
    
    def check_quantized_param(self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: Dict[str, Any], **kwargs) -> bool:
        ...
    
    def create_quantized_param(self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, target_device: torch.device, state_dict: Dict[str, Any], unexpected_keys: List[str]): # -> None:
        """
        Each nn.Linear layer that needs to be quantized is processsed here.
        First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.
        """
        ...
    
    def is_serializable(self, safe_serialization=...): # -> bool:
        ...
    
    @property
    def is_trainable(self): # -> bool:
        ...
    


