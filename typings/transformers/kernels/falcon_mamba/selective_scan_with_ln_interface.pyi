"""
This type stub file was generated by pyright.
"""

import torch
import mamba_ssm
from torch.cuda.amp import custom_bwd, custom_fwd

if hasattr(mamba_ssm.ops.triton, "layernorm"):
    ...
else:
    ...
class SelectiveScanFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx, u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...): # -> tuple[Any, Any]:
        ...
    
    @staticmethod
    def backward(ctx, dout, *args): # -> tuple[Any, Any, Any, Any, Any, Any | None, Any | None, Any | None, None, None]:
        ...
    


def rms_norm_forward(x, weight, bias, eps=..., is_rms_norm=...):
    ...

def selective_scan_fn(u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...): # -> Any | None:
    """if return_last_state is True, returns (out, last_state)
    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is
    not considered in the backward pass.
    """
    ...

def selective_scan_ref(u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...): # -> Tensor | tuple[Tensor | Any, Any | None]:
    """
    u: r(B D L)
    delta: r(B D L)
    A: c(D N) or r(D N)
    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    D: r(D)
    z: r(B D L)
    delta_bias: r(D), fp32

    out: r(B D L)
    last_state (optional): r(B D dstate) or c(B D dstate)
    """
    ...

class MambaInnerFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B=..., C=..., D=..., delta_bias=..., B_proj_bias=..., C_proj_bias=..., delta_softplus=..., checkpoint_lvl=..., b_rms_weight=..., c_rms_weight=..., dt_rms_weight=..., b_c_dt_rms_eps=...): # -> Tensor:
        """
        xz: (batch, dim, seqlen)
        """
        ...
    
    @staticmethod
    @custom_bwd
    def backward(ctx, dout): # -> tuple[torch._tensor.Tensor, Any, Any | None, torch.functional.Tensor, torch.functional.Tensor, torch.functional.Tensor, Any | None, Any, Any | None, Any | None, Any | None, Any | None, Any | None, Any | None, None, None, None, None, None, None]:
        ...
    


def mamba_inner_fn(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight, out_proj_weight, out_proj_bias, A, B=..., C=..., D=..., delta_bias=..., B_proj_bias=..., C_proj_bias=..., delta_softplus=..., checkpoint_lvl=..., b_rms_weight=..., c_rms_weight=..., dt_rms_weight=..., b_c_dt_rms_eps=...): # -> Any | None:
    ...

