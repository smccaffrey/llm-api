"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from functools import lru_cache
from typing import List, Optional, Tuple, Union
from torch import Tensor, nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable
from ...file_utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from ...modeling_utils import PreTrainedModel
from .configuration_omdet_turbo import OmDetTurboConfig

"""PyTorch OmDet-Turbo model."""
MultiScaleDeformableAttention = ...
logger = ...
_CONFIG_FOR_DOC = ...
@dataclass
class OmDetTurboEncoderOutput(ModelOutput):
    """
    Base class for outputs of the OmDetTurboHybridEncoder.

    Args:
        last_hidden_state (`torch.FloatTensor`):
            Last hidden states of the encoder.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
            sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.
        extracted_states (`Tuple[torch.FloatTensor]`):
            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.
    """
    last_hidden_state: torch.FloatTensor = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    attentions: Optional[Tuple[torch.FloatTensor]] = ...
    extracted_states: Tuple[torch.FloatTensor] = ...


@dataclass
class OmDetTurboDecoderOutput(ModelOutput):
    """
    Base class for outputs of the OmDetTurboDecoder.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the decoder.
        decoder_coords (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The predicted coordinates of the objects.
        decoder_classes (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):
            The predicted classes of the objects.
        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The predicted coordinates of the objects from the encoder.
        encoder_class_logits (`Tuple[torch.FloatTensor]`) of shape `(batch_size, num_queries, num_classes)`:
            The predicted class of the objects from the encoder.
        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The initial reference points.
        intermediate_reference_points (`Tuple[Tuple[torch.FloatTensor]]`):
            The intermediate reference points.
        hidden_states (`Optional[Tuple[torch.FloatTensor]]`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape
            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
            plus the initial embedding outputs.
        attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,
            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the
            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.
    """
    last_hidden_state: torch.FloatTensor = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = ...
    decoder_coords: torch.FloatTensor = ...
    decoder_classes: torch.FloatTensor = ...
    encoder_coord_logits: torch.FloatTensor = ...
    encoder_class_logits: Tuple[torch.FloatTensor] = ...
    init_reference_points: torch.FloatTensor = ...
    intermediate_reference_points: Tuple[Tuple[torch.FloatTensor]] = ...


@dataclass
class OmDetTurboObjectDetectionOutput(ModelOutput):
    """
    Output type of [`OmDetTurboObjectDetectionOutput`].

    Args:
        loss (`torch.FloatTensor`):
            The loss value.
        decoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The predicted coordinates logits of the objects.
        decoder_class_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):
            The predicted class of the objects.
        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The initial reference points.
        intermediate_reference_points (`Tuple[Tuple[torch.FloatTensor]]`):
            The intermediate reference points.
        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
            The predicted coordinates of the objects from the encoder.
        encoder_class_logits (`Tuple[torch.FloatTensor]`):
            The predicted class of the objects from the encoder.
        encoder_extracted_states (`torch.FloatTensor`):
            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.
        decoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape
            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
            plus the initial embedding outputs.
        decoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):
            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,
            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the
            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.
        encoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape
            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
            plus the initial embedding outputs.
        encoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):
            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,
            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the
            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.
    """
    loss: torch.FloatTensor = ...
    decoder_coord_logits: torch.FloatTensor = ...
    decoder_class_logits: torch.FloatTensor = ...
    init_reference_points: torch.FloatTensor = ...
    intermediate_reference_points: Optional[Tuple[Tuple[torch.FloatTensor]]] = ...
    encoder_coord_logits: torch.FloatTensor = ...
    encoder_class_logits: Tuple[torch.FloatTensor] = ...
    encoder_extracted_states: torch.FloatTensor = ...
    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = ...
    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = ...


def load_cuda_kernels(): # -> None:
    ...

def multi_scale_deformable_attention(value: Tensor, value_spatial_shapes: Union[Tensor, List[Tuple]], sampling_locations: Tensor, attention_weights: Tensor) -> Tensor:
    ...

class OmDetTurboLRUCache:
    def __init__(self, capacity: int) -> None:
        ...
    
    def has(self, key) -> bool:
        ...
    
    def get(self, key): # -> None:
        """
        Get the value of the key if the key exists in the cache, otherwise return None.
        Move the key to the end of the cache to show that it was recently used.
        """
        ...
    
    def put(self, key, value) -> None:
        """
        Add the key-value pair to the cache.
        Move the key to the end of the cache to show that it was recently used.
        If the cache is full, remove the first key (least recently used).
        """
        ...
    


class OmDetTurboLanguageBackbone(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def forward(self, hidden_states, mask=..., encode_type=...): # -> tuple[Any, Any]:
        ...
    


class OmDetTurboVisionBackbone(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def forward(self, pixel_values): # -> list[Any]:
        ...
    


class MultiScaleDeformableAttentionFunction(Function):
    @staticmethod
    def forward(context, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step):
        ...
    
    @staticmethod
    @once_differentiable
    def backward(context, grad_output): # -> tuple[Any, None, None, Any, Any, None]:
        ...
    


class OmDetTurboMultiscaleDeformableAttention(nn.Module):
    """
    Multiscale deformable attention as proposed in Deformable DETR.
    """
    def __init__(self, config: OmDetTurboConfig, num_heads: int, n_points: int) -> None:
        ...
    
    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]): # -> Tensor:
        ...
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = ..., encoder_hidden_states=..., encoder_attention_mask=..., position_embeddings: Optional[torch.Tensor] = ..., reference_points=..., spatial_shapes=..., spatial_shapes_list=..., level_start_index=..., output_attentions: bool = ...): # -> tuple[Any, Tensor]:
        ...
    


class OmDetTurboConvNormLayer(nn.Module):
    def __init__(self, config, in_channels, out_channels, kernel_size, stride, padding=..., activation=...) -> None:
        ...
    
    def forward(self, hidden_state): # -> Any:
        ...
    


class OmDetTurboRepVggBlock(nn.Module):
    """
    RepVGG architecture block introduced by the work "RepVGG: Making VGG-style ConvNets Great Again".
    """
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class OmDetTurboCSPRepLayer(nn.Module):
    """
    Cross Stage Partial (CSP) network layer with RepVGG blocks.
    """
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def forward(self, hidden_state): # -> Any:
        ...
    


class OmDetTurboMultiheadAttention(nn.Module):
    """Equivalent implementation of nn.MultiheadAttention with `batch_first=True`."""
    def __init__(self, config, hidden_size, num_attention_heads, dropout) -> None:
        ...
    
    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
        ...
    
    def forward(self, queries: torch.Tensor, keys: torch.Tensor, values: torch.Tensor, attention_mask: Optional[torch.FloatTensor] = ..., output_attentions: Optional[bool] = ...) -> Tuple[torch.Tensor]:
        ...
    


class OmDetTurboEncoderLayer(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    @staticmethod
    def with_pos_embed(tensor, pos_embed):
        ...
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, position_embeddings: torch.Tensor = ..., output_attentions: bool = ...): # -> tuple[Tensor, Tensor | None] | tuple[Tensor]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative
                values.
            position_embeddings (`torch.FloatTensor`, *optional*):
                Object queries (also called content embeddings), to be added to the hidden states.
            output_attentions (`bool`, *optional*, defaults to `False`):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        ...
    


class OmDetTurboEncoder(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def forward(self, src, src_mask=..., pos_embed=..., output_attentions: bool = ...) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:
        ...
    


class OmDetTurboHybridEncoder(nn.Module):
    """
    Encoder consisting of channel projection layers, a set of `OmDetTurboEncoder`, a top-down Feature Pyramid Network
    (FPN) and a bottom-up Path Aggregation Network (PAN). More details on the paper: https://arxiv.org/abs/2304.08069

    Args:
        config: OmDetTurboConfig
    """
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    @staticmethod
    def build_2d_sincos_position_embedding(width, height, embed_dim=..., temperature=..., device=..., dtype=...): # -> Tensor:
        ...
    
    def forward(self, inputs_embeddings=..., output_attentions=..., output_hidden_states=..., return_dict=...): # -> tuple[Any, tuple[Any, ...] | Any | tuple[()] | None, tuple[()] | tuple[Any, ...] | None, list[Any]] | OmDetTurboEncoderOutput:
        r"""
        Args:
            inputs_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                Flattened feature map (output of the backbone + projection layers) that is passed to the encoder.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.
        """
        ...
    


class OmDetTurboMLPWithDropout(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class OmDetTurboMLP(nn.Module):
    """Very simple multi-layer perceptron (also called FFN)"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers) -> None:
        ...
    
    def forward(self, x): # -> Tensor | Any:
        ...
    


class OmDetTurboResidualLayer(nn.Module):
    """
    A residual connection followed by a layer norm.
    """
    def __init__(self, config) -> None:
        ...
    
    def forward(self, x, y): # -> Any:
        ...
    


class OmDetTurboTaskEncoder(nn.Module):
    def __init__(self, config) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


class OmDetTurboDeformableTransformerDecoderLayer(nn.Module):
    """
    A single layer of the Deformable Transformer Decoder.
    """
    def __init__(self, config) -> None:
        ...
    
    @staticmethod
    def with_pos_embed(tensor, pos):
        ...
    
    def forward(self, decoder_embeddings, task_features, reference_points, vision_features, vision_shapes, vision_shapes_list, level_start_index=..., attention_mask=..., padding_mask=..., query_position=..., output_attentions=..., output_hidden_states=...): # -> tuple[Any, Any, Any | None, Any | None]:
        ...
    


class OmDetTurboPreTrainedModel(PreTrainedModel):
    config_class = OmDetTurboConfig
    base_model_prefix = ...
    main_input_name = ...
    def get_cached_class_embeddings(self, classes_input_ids, classes_attention_mask): # -> Tensor:
        ...
    
    def get_cached_task_embeddings(self, tasks_input_ids, tasks_attention_mask): # -> tuple[Tensor, Tensor]:
        ...
    
    def get_language_embedding(self, classes_input_ids, classes_attention_mask, tasks_input_ids, tasks_attention_mask, classes_structure): # -> tuple[Tensor, Tensor, Tensor]:
        ...
    


OMDET_TURBO_START_DOCSTRING = ...
OMDET_TURBO_INPUTS_DOCSTRING = ...
def get_class_similarity(class_distance_type, cls_feature, class_proj): # -> Tensor:
    ...

class OmDetTurboDecoder(OmDetTurboPreTrainedModel):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    @lru_cache(maxsize=32)
    def generate_anchors(self, spatial_shapes=..., grid_size=..., device=..., dtype=...): # -> tuple[Tensor, Tensor]:
        ...
    
    def forward(self, vision_features, class_features, task_features, task_mask, output_attentions=..., output_hidden_states=..., return_dict=...): # -> tuple[Any | Tensor, tuple[Tensor | Any, ...] | Any | tuple[Any, ...] | tuple[()] | None, tuple[tuple[Any, ...] | Any | tuple[()] | None, ...] | Any | tuple[()] | None, Tensor, Tensor, Any, Tensor, Tensor, Tensor | Any] | OmDetTurboDecoderOutput:
        """
        Args:
            vision_features (`torch.FloatTensor`): The sequence of vision features. shape depends on the vision
                backbone.
            class_features (`torch.FloatTensor`): The sequence of class features of shape
                `(class_sequence_length, batch_size, class_embed_dim)`.
            task_features (`torch.FloatTensor`): The sequence of task features of shape
                `(task_sequence_length, batch_size, decoder_hidden_dim)`.
            task_mask (`torch.LongTensor`): The mask for the task features of shape `(batch_size, task_sequence_length)`.
            output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all attention
                layers. See `attentions` under returned tensors for more detail.
            output_hidden_states (`bool`, *optional*): Whether or not to return the hidden states of all layers. See
                `hidden_states` under returned tensors for more detail.
            return_dict (`bool`, *optional*): Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain
                tuple.
        """
        ...
    


@add_start_docstrings("""
    OmDetTurbo Model (consisting of a vision and a text backbone, and encoder-decoder architecture) outputting
    bounding boxes and classes scores for tasks such as COCO detection.
    """, OMDET_TURBO_START_DOCSTRING)
class OmDetTurboForObjectDetection(OmDetTurboPreTrainedModel):
    def __init__(self, config: OmDetTurboConfig) -> None:
        ...
    
    def get_input_embeddings(self):
        ...
    
    def set_input_embeddings(self, value): # -> None:
        ...
    
    def resize_token_embeddings(self, new_num_tokens: Optional[int] = ..., pad_to_multiple_of=...) -> nn.Embedding:
        ...
    
    @add_start_docstrings_to_model_forward(OMDET_TURBO_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=OmDetTurboObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, pixel_values: Tensor, classes_input_ids: Tensor, classes_attention_mask: Tensor, tasks_input_ids: Tensor, tasks_attention_mask: Tensor, classes_structure: Tensor, labels: Optional[Tensor] = ..., output_attentions=..., output_hidden_states=..., return_dict=...) -> Union[Tuple[torch.FloatTensor], OmDetTurboObjectDetectionOutput]:
        r"""
        Returns:

        Examples:

        ```python
        >>> import requests
        >>> from PIL import Image

        >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection

        >>> processor = AutoProcessor.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")
        >>> model = OmDetTurboForObjectDetection.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> classes = ["cat", "remote"]
        >>> task = "Detect {}.".format(", ".join(classes))
        >>> inputs = processor(image, text=classes, task=task, return_tensors="pt")

        >>> outputs = model(**inputs)

        >>> # convert outputs (bounding boxes and class logits)
        >>> results = processor.post_process_grounded_object_detection(
        ...     outputs,
        ...     classes=classes,
        ...     target_sizes=[image.size[::-1]],
        ...     score_threshold=0.3,
        ...     nms_threshold=0.3,
        >>> )[0]
        >>> for score, class_name, box in zip(results["scores"], results["classes"], results["boxes"]):
        ...     box = [round(i, 1) for i in box.tolist()]
        ...     print(
        ...         f"Detected {class_name} with confidence "
        ...         f"{round(score.item(), 2)} at location {box}"
        ...     )
        Detected remote with confidence 0.76 at location [39.9, 71.3, 176.5, 117.9]
        Detected cat with confidence 0.72 at location [345.1, 22.5, 639.7, 371.9]
        Detected cat with confidence 0.65 at location [12.7, 53.8, 315.5, 475.3]
        Detected remote with confidence 0.57 at location [333.4, 75.6, 370.7, 187.0]
        ```"""
        ...
    


