"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Tuple, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward
from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available
from .configuration_mamba2 import Mamba2Config
from mamba_ssm.ops.triton.selective_state_update import selective_state_update

"""PyTorch MAMBA2 model."""
logger = ...
if is_mamba_2_ssm_available():
    ...
else:
    selective_state_update = ...
if is_causal_conv1d_available():
    ...
else:
    ...
is_fast_path_available = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...
def pad_tensor_by_size(input_tensor: torch.Tensor, pad_size: int): # -> Tensor:
    """
    Padding x tensor with `pad_size` on the seq_len dim (dim=1)

    Assumes that we only have tensors of either size 4 or 3
    """
    ...

def reshape_into_chunks(input_tensor, pad_size, chunk_size): # -> Tensor:
    """
    Padding input_tensor with `pad_size` on the seq_len dim (dim=1) and
    simultaneously splitting it into chunk sequences.

    Assumes that we only have tensors of either size 4 or 3
    """
    ...

def segment_sum(input_tensor): # -> Tensor:
    """
    More stable segment sum calculation. Uses cumulative sums and masking instead of direct subtractions.
    """
    ...

class Mamba2Cache:
    """
    Arguments:
        config: Mamba2Config
        batch_size: int
        dtype: torch.dtype
        device: torch.device

    Attributes:
        seqlen_offset: int
        dtype: torch.dtype
        conv_states: Dict[int, torch.Tensor] # layer_idx -> [batch_size, intermediate_size, conv_kernel_size]
        ssm_states: Dict[int, torch.Tensor] # layer_idx -> [batch_size, intermediate_size, ssm_state_size]
    """
    def __init__(self, config: Mamba2Config, batch_size: int, dtype: torch.dtype = ..., device: Optional[str] = ...) -> None:
        ...
    
    def update_conv_state(self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor) -> torch.Tensor:
        ...
    
    def reset(self): # -> None:
        ...
    


class MambaRMSNormGated(torch.nn.Module):
    def __init__(self, hidden_size, eps=...) -> None:
        ...
    
    def forward(self, hidden_states, gate=...):
        ...
    


class Mamba2Mixer(nn.Module):
    """
    Compute ∆, A, B, C, and D the state space parameters and compute the `contextualized_states`.
    A, D are input independent (see Mamba paper [1] Section 3.5.2 "Interpretation of A" for why A isn't selective)
    ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,
    and is why Mamba is called **selective** state spaces)
    """
    def __init__(self, config: Mamba2Config, layer_idx: int) -> None:
        ...
    
    def cuda_kernels_forward(self, hidden_states: torch.Tensor, cache_params: Optional[Mamba2Cache] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ...): # -> Any:
        ...
    
    def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ...): # -> Any:
        ...
    
    def forward(self, hidden_states, cache_params: Optional[Mamba2Cache] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ...): # -> Any:
        ...
    


class Mamba2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=...) -> None:
        """
        Mamba2RMSNorm is equivalent to T5LayerNorm and LlamaRMSNorm
        """
        ...
    
    def forward(self, hidden_states):
        ...
    


class Mamba2Block(nn.Module):
    def __init__(self, config, layer_idx) -> None:
        ...
    
    def forward(self, hidden_states, cache_params: Optional[Mamba2Cache] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ...):
        ...
    


class Mamba2PreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = Mamba2Config
    base_model_prefix = ...
    _no_split_modules = ...
    supports_gradient_checkpointing = ...
    _is_stateful = ...


@dataclass
class Mamba2Output(ModelOutput):
    """
    Class for the MAMBA2 model outputs.

    Args:
        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        cache_params (`Mamba2Cache`):
            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to
            avoid providing the old `input_ids`.

            Includes both the State space model state matrices after the selective scan, and the Convolutional states
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
    """
    last_hidden_state: Optional[torch.FloatTensor] = ...
    cache_params: Optional[Mamba2Cache] = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...


@dataclass
class Mamba2CausalLMOutput(ModelOutput):
    """
    Base class for causal language model (or autoregressive) outputs.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
            Language modeling loss (for next-token prediction).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
        cache_params (`Mamba2Cache`):
            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to
            avoid providing the old `input_ids`.

            Includes both the State space model state matrices after the selective scan, and the Convolutional states
        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +
            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
    """
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    cache_params: Optional[Mamba2Cache] = ...
    hidden_states: Optional[Tuple[torch.FloatTensor]] = ...


MAMBA2_START_DOCSTRING = ...
MAMBA2_INPUTS_DOCSTRING = ...
@add_start_docstrings("The bare MAMBA2 Model transformer outputting raw hidden-states without any specific head on top.", MAMBA2_START_DOCSTRING)
class Mamba2Model(Mamba2PreTrainedModel):
    def __init__(self, config) -> None:
        ...
    
    def load_hook(self, state_dict, prefix, *args): # -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    @add_start_docstrings_to_model_forward(MAMBA2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(checkpoint=_CHECKPOINT_FOR_DOC, output_type=Mamba2Output, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Optional[torch.LongTensor] = ..., inputs_embeds: Optional[torch.LongTensor] = ..., cache_params: Optional[Mamba2Cache] = ..., use_cache: Optional[bool] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ..., **kwargs) -> Union[Tuple, Mamba2Output]:
        ...
    


@add_start_docstrings("""
    The MAMBA2 Model transformer with a language modeling head on top (linear layer with weights not tied to the input
    embeddings).
    """, MAMBA2_START_DOCSTRING)
class Mamba2ForCausalLM(Mamba2PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config) -> None:
        ...
    
    def get_output_embeddings(self): # -> Linear:
        ...
    
    def set_output_embeddings(self, new_embeddings): # -> None:
        ...
    
    def get_input_embeddings(self): # -> Embedding:
        ...
    
    def set_input_embeddings(self, new_embeddings): # -> None:
        ...
    
    def prepare_inputs_for_generation(self, input_ids, inputs_embeds=..., use_cache=..., cache_params: Optional[Mamba2Cache] = ..., cache_position: Optional[torch.LongTensor] = ..., attention_mask: Optional[torch.Tensor] = ..., **kwargs): # -> dict[str, Any]:
        ...
    
    @add_start_docstrings_to_model_forward(MAMBA2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(checkpoint=_CHECKPOINT_FOR_DOC, output_type=Mamba2CausalLMOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids: Optional[torch.LongTensor] = ..., inputs_embeds: Optional[torch.FloatTensor] = ..., cache_params: Optional[Mamba2Cache] = ..., labels: Optional[torch.LongTensor] = ..., output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ..., use_cache: Optional[bool] = ..., cache_position: Optional[torch.Tensor] = ..., attention_mask: Optional[torch.Tensor] = ..., **kwargs) -> Union[Tuple, Mamba2CausalLMOutput]:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        """
        ...
    


